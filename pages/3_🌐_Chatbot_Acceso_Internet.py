import sys
import os

# A√±adir el directorio ra√≠z al path para poder importar utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import utils
import streamlit as st
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.tools import Tool

# Importar herramienta de b√∫squeda con respaldo
import search_utils

# Importar nuestro callback personalizado
from custom_callbacks import CustomStreamlitCallbackHandler

st.set_page_config(page_title="ChatNet", page_icon="üåê")
st.header("Chatbot con Acceso a Internet")
st.write(
    "Equipado con acceso a internet, permite a los usuarios hacer preguntas sobre eventos recientes"
)


class InternetChatbot:
    def __init__(self):
        utils.sync_st_session()
        self.llm = utils.configure_llm()

    def setup_agent(self):
        # Usar directamente nuestra herramienta de b√∫squeda con respaldo
        # Esto evita mostrar errores al usuario y garantiza que siempre tengamos una respuesta
        fallback_search = search_utils.get_search_tool()
        search_tool = Tool(
            name="InternetSearch",
            func=fallback_search.run,
            description="√ötil cuando necesitas responder preguntas sobre eventos actuales. Debes hacer preguntas espec√≠ficas",
        )

        # Configurar herramientas
        tools = [search_tool]

        # Usar el prompt est√°ndar de React
        prompt = hub.pull("hwchase17/react-chat")

        # Setup LLM and Agent
        memory = ConversationBufferMemory(memory_key="chat_history")
        agent = create_react_agent(self.llm, tools, prompt)

        # Configurar el agente para que no sea verbose (ocultar la cadena de pensamiento)
        agent_executor = AgentExecutor(
            agent=agent, tools=tools, memory=memory, verbose=False
        )
        return agent_executor, memory

    @utils.enable_chat_history
    def main(self):
        # Mostrar informaci√≥n sobre las herramientas de b√∫squeda alternativas
        with st.sidebar.expander("‚ÑπÔ∏è Informaci√≥n sobre b√∫squedas"):
            st.markdown(
                """
            ### Herramientas de b√∫squeda gratuitas

            Este chatbot utiliza m√∫ltiples herramientas de b√∫squeda gratuitas con un sistema de respaldo:

            1. **DuckDuckGo** (principal)
            2. **Google** (respaldo mediante scraping)
            3. **Bing** (respaldo mediante scraping)
            4. **DuckDuckGo HTML** (respaldo final mediante scraping)

            Si experimentas errores de "rate limit", el sistema intentar√° usar autom√°ticamente los m√©todos alternativos.

            > **Nota**: Todos los m√©todos de b√∫squeda son gratuitos y no requieren API keys.
            """
            )

            # Consejos para mejorar las b√∫squedas
            st.subheader("Consejos para mejorar las b√∫squedas")
            st.markdown(
                """
            - Haz preguntas espec√≠ficas y concretas
            - Incluye fechas, nombres completos y detalles relevantes
            - Evita preguntas muy generales
            - Si recibes un error de l√≠mite de tasa, espera unos minutos e intenta de nuevo
            - Reformula tu pregunta si no obtienes resultados satisfactorios
            """
            )

        # Configurar el agente con manejo de errores
        try:
            agent_executor, memory = self.setup_agent()
        except Exception as e:
            st.error(f"Error al configurar el agente: {str(e)}")
            st.info("Intenta recargar la p√°gina o verifica tu conexi√≥n a internet.")
            st.stop()

        # Interfaz de chat
        user_query = st.chat_input(
            placeholder="¬°Hazme una pregunta sobre eventos actuales!"
        )
        if user_query:
            utils.display_msg(user_query, "user")
            with st.chat_message("assistant"):
                # Usar nuestro callback personalizado que oculta la cadena de pensamiento
                custom_cb = CustomStreamlitCallbackHandler(st.container())
                # Mostrar indicador de carga
                with st.status("Buscando informaci√≥n...", expanded=False) as status:
                    try:
                        result = agent_executor.invoke(
                            {
                                "input": user_query,
                                "chat_history": memory.chat_memory.messages,
                            },
                            {"callbacks": [custom_cb]},
                        )
                        response = result["output"]
                        status.update(
                            label="¬°Informaci√≥n encontrada!", state="complete"
                        )

                        # Mostrar respuesta
                        st.session_state.messages.append(
                            {"role": "assistant", "content": response}
                        )
                        st.write(response)

                    except Exception as e:
                        # Registrar el error en el log para depuraci√≥n (no visible para el usuario)
                        import logging

                        logging.error(f"Error en la b√∫squeda: {str(e)}")

                        # Intentar obtener una respuesta directamente de la herramienta de b√∫squeda
                        fallback_search = search_utils.get_search_tool()
                        raw_search_results = fallback_search.run(user_query)

                        # Procesar los resultados con el LLM para obtener una respuesta m√°s natural
                        try:
                            # Crear un prompt para que el LLM procese los resultados
                            prompt_template = f"""Bas√°ndote en la siguiente informaci√≥n de b√∫squeda, responde a la pregunta: '{user_query}'

                            RESULTADOS DE B√öSQUEDA:
                            {raw_search_results}

                            Proporciona una respuesta clara, concisa y bien estructurada. Si la informaci√≥n no es suficiente, ind√≠calo.
                            No menciones que est√°s basando tu respuesta en resultados de b√∫squeda. Responde como si tuvieras el conocimiento directamente."""

                            # Usar el LLM para procesar los resultados
                            processed_response = self.llm.invoke(
                                prompt_template
                            ).content
                        except Exception as llm_error:
                            # Si falla el procesamiento con el LLM, usar los resultados crudos
                            logging.error(
                                f"Error al procesar con LLM: {str(llm_error)}"
                            )
                            processed_response = raw_search_results

                        # Actualizar el estado del indicador de carga
                        status.update(
                            label="¬°Informaci√≥n encontrada!", state="complete"
                        )

                        # Mostrar la respuesta procesada al usuario
                        st.session_state.messages.append(
                            {"role": "assistant", "content": processed_response}
                        )
                        st.write(processed_response)


if __name__ == "__main__":
    obj = InternetChatbot()
    obj.main()
